input {
  # Consume logs from Kafka topic
  kafka {
    bootstrap_servers => "${KAFKA_BOOTSTRAP_SERVERS}"
    topics => ["application.logs"]
    group_id => "logstash-consumer-group"
    codec => "json"
    consumer_threads => 3
    decorate_events => true
  }
}

filter {
  # Parse JSON logs
  if [message] {
    json {
      source => "message"
      target => "log"
    }
  }

  # Extract fields from parsed JSON
  if [log] {
    mutate {
      add_field => {
        "service_name" => "%{[log][service_name]}"
        "log_level" => "%{[log][level]}"
        "trace_id" => "%{[log][trace_id]}"
        "span_id" => "%{[log][span_id]}"
        "logger_name" => "%{[log][logger_name]}"
      }
    }
  }

  # Add timestamp
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }

  # Grok pattern for stack traces
  if [log][message] =~ /Exception|Error/ {
    mutate {
      add_tag => ["error"]
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["@version", "kafka"]
  }
}

output {
  # Send to Elasticsearch with daily index rotation
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS}"]
    index => "logs-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }

  # Debug output (optional, comment out in production)
  # stdout {
  #   codec => rubydebug
  # }
}
